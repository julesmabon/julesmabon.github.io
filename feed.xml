<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://www.jmabon.fr//feed.xml" rel="self" type="application/atom+xml" /><link href="https://www.jmabon.fr//" rel="alternate" type="text/html" /><updated>2024-01-23T11:06:54+01:00</updated><id>https://www.jmabon.fr//feed.xml</id><title type="html">J. Mabon</title><subtitle>Homepage of J. Mabon</subtitle><author><name>J. Mabon</name></author><entry><title type="html">PhD defense</title><link href="https://www.jmabon.fr//research/2023/12/20/phd_defense.html" rel="alternate" type="text/html" title="PhD defense" /><published>2023-12-20T12:33:33+01:00</published><updated>2023-12-20T12:33:33+01:00</updated><id>https://www.jmabon.fr//research/2023/12/20/phd_defense</id><content type="html" xml:base="https://www.jmabon.fr//research/2023/12/20/phd_defense.html">&lt;p&gt;I successfully defended my PhD in &lt;em&gt;Automatique, traitement du signal et des images&lt;/em&gt; (Automatic control, signal and image processing) titled : &lt;em&gt;‚ÄúLearning Stochastic geometry models and convolutional neural networks. Application to multiple object detection in aerospatial data sets.‚Äù&lt;/em&gt;, at Inria d‚ÄôUniversit√© C√¥te d‚ÄôAzur, on December 20, 2023.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://theses.hal.science/tel-04404849&quot;&gt;üìë Thesis manuscript üîó&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.jmabon.fr//media/presentation.pdf&quot;&gt;üìä Presentation slides üîó&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;üìπ Recording of the defense
  &lt;em&gt;(presentation in üá¨üáß and questions in üá´üá∑ and üá¨üáß)&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/fBDe1aaeRE8?si=QjgKq7VkJYDa0rGx&amp;amp;start=20&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;</content><author><name>J. Mabon</name></author><category term="research" /><summary type="html">I successfully defended my PhD in Automatique, traitement du signal et des images (Automatic control, signal and image processing) titled : ‚ÄúLearning Stochastic geometry models and convolutional neural networks. Application to multiple object detection in aerospatial data sets.‚Äù, at Inria d‚ÄôUniversit√© C√¥te d‚ÄôAzur, on December 20, 2023. üìë Thesis manuscript üîó üìä Presentation slides üîó üìπ Recording of the defense (presentation in üá¨üáß and questions in üá´üá∑ and üá¨üáß)</summary></entry><entry><title type="html">Photography II</title><link href="https://www.jmabon.fr//creative/2023/09/04/photography2.html" rel="alternate" type="text/html" title="Photography II" /><published>2023-09-04T10:00:00+02:00</published><updated>2023-09-04T10:00:00+02:00</updated><id>https://www.jmabon.fr//creative/2023/09/04/photography2</id><content type="html" xml:base="https://www.jmabon.fr//creative/2023/09/04/photography2.html">&lt;p&gt;Some shots from end 2021-2023, from me and my camera&lt;/p&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/pw/ADCreHex_sx3n2q73WMh4Xy_F9ag99MbxiZivuHk7AIsCxs4GIUbLAfL9dj8XjIOxu28BnV0PV9j9F7C3mxurZT5EVyq9UmmTiP30dmOHM9AxkK0flSCiNo-FiRbkVaIM09WSc_71xUdgX0U2KdVcDQGfItZIg=w2200-h863-s-no&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;Chamonix apr√®s la pluie, 2021&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/pw/ADCreHev0gtrMtbOfynwZCRldp3IT9YqTO9EkSKsWZxQp8rZkmSxEg2ZxHqWuXxhSLalPLu9x_H_GA7Vk8Wo7TptvCvfGqm6xlgwiATFQ74h2DSDCa5zUNQz9Gq2N4GOtvhxkMfThO5Qk2Wmq3RUsxjbOyC59Q=w2200-h1237-s-no&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;Chamonix avant la nuit, 2021&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/pw/ADCreHeV_G_quRAy4k10i221UYcBVHthtDIOZwUnCNyOM7IQI8173yBv7kW8rf4-qVc0y2ArhhUwwt3c9KVBpfhdLdx4XPqKs2QKIgvVlzLBcKDyzk6rIN-XIBn9HhmYCuoSgbQieily1dOKF1R1Cha6dKoddQ=w2200-h1237-s-no&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;Cimeti√®re de Nice, 2021&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/pw/ADCreHdUbj9ZUmctNVYe12pRDe0pV6lEanQwjUP8oWZzrZHuyR03s2tvBBe-EmlhkW4KNzWj2-08uoPN_8AoeVCOESZKDjb-L3ppseghF1uMLsnWn0K_QrMtMlojKXWsxqnRwlR-Su2G4uTV3dvksr1HvXJ8Jw=w2200-h1237-s-no&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;&quot;Where&apos;s this drone ?&quot;, Cap d&apos;Antibes, 2021&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/pw/ADCreHfb5lFzISfvTwN5Ft_rfcEmgISJjq9F2f1vohbPOkfq-ZuVth372XDlx7FcXasNaLPMOod6zkpc9lHzqwVIe_Hu7-dHSG6soT8EuYLmjHQKn5pMP2K4UQGMDyQXYVRLNW1XmUHoKhM8nQKr5X9ObPYHCw=w2200-h1467-s-no&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;Parc naturel d√©partemental de Vaugrenier, 2022&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/pw/ADCreHdNe1Glqu0jVWiyoSVx-LbvSKLMqzRGs3T4c5nfj72I15RwADgqhciZNDJpYR5CuvHzAvHzxqPq32Zq0GE-nfi3zaCfiOa69em23BXxRp3xS6S60lhNyV_awY0pemRtBcaDJ1dU_XgcAq90iWbxwZdozA=w2200-h1237-s-no&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;Nice, Promenade, 2022&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/pw/ADCreHfqjKvsHyRqe7Ib4QRdUqh3fpgtvErXXjrFOBkAPXAeOWpGWeSVKUg9shuyLw2STh-20VTFhuoCp75wAXlfCdnD9YEioj5NRWINqgXDlBrnJmJ7mBU3R5UDmZYZY7S28KsosrDSCK6rv7odzVwjc403bA=w2200-h1237-s-no&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;Nice, Promenade, 2022&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/pw/ADCreHezetAj-1Oq4CH9JG509Q0QBMV3gOJ6Nmh3xIAvGdxR8EyL3PBemC4BxAa8CPUdIPguzjm-Ut9_EybGqSbR3jcwCmKUQy3FPj0jaYLOmk-DLD2iRWJZ504fkHq8W9kO8ouF5JH8IT9z2jKVp74E4k-fOQ=w1115-h1487-s-no&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;Porto, 2022&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/pw/ADCreHcZWJeUDlS_Qrv1uTYE9q35b1vVXnUkiNk5RkexSJX8FNb_AswaP5mb1Go-O6YrMWZNFAjW3PdSuX-ssIF4SUCqPQlNS5kT6l19dIItYCIYfae2SuUhKzj4aC2vScwx2f6SgSkbo9MyIremR4FDpGiCtg=w2200-h1467-s-no&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;&quot;Attention √† la mousse&quot;, Cap d&apos;Antibes, 2023&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/pw/ADCreHe7Avf-ay_aj4DIWvFseBQUlBJ_VzihONjEtGmG96Xifhs6dlUOsaQ5lrIeB8Mx_oY-skBwz9Vluj_G7mc6kiNNndN0JTyOViQv2lgv-ZTeimQ08cX2mmWydL1SYEzijgfBDEvlarbD98JbH9wBge7clQ=w1983-h1487-s-no&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;üòæ‚Äã, 2023&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/pw/ADCreHf0K_W8En-zaVv_mjRHVAYAWwiEpEGJLF0uklCmxqyCXoDrmJzlHUrwJuID72kZ0DtXVVQdai4GUSdIJAmUKftV4-rypANAWZq92-YmObaIT04IKIn-W9Yd0bIQ2Wu08TH_VCROf_fN7_JJzN92vvTEGQ=w1983-h1487-s-no&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;üê≤‚Äãü™∞‚Äã, 2023&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;</content><author><name>J. Mabon</name></author><category term="creative" /><summary type="html">Some shots from end 2021-2023, from me and my camera</summary></entry><entry><title type="html">Works in progress</title><link href="https://www.jmabon.fr//creative/2023/09/04/wip1.html" rel="alternate" type="text/html" title="Works in progress" /><published>2023-09-04T10:00:00+02:00</published><updated>2023-09-04T10:00:00+02:00</updated><id>https://www.jmabon.fr//creative/2023/09/04/wip1</id><content type="html" xml:base="https://www.jmabon.fr//creative/2023/09/04/wip1.html">&lt;p&gt;Sometimes you make things but it is never done. Some of it goes here.
These images are made on the computer with Blender.&lt;/p&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/pw/ADCreHfBXMZHUBb0PbUzQJmR75Exw95JjuE_0VHE01PpOUKLAnjU_Bb8V5zZd9emhWQjfDd12aVW25nPOga1AVYOTf2hqzpwaQF7Dc0OEEeyAXCbMNlRHHBpy316SSX5jEsXsCjobeDccWDNmT0XR7cZScllQw=w2200-h863-s-no&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;Sometime be like driving your tracteur jaune, Sep 2021&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/pw/ADCreHfCLC2Lqlw_8FsdZx0M5W-IHIH_-inhnB8MKm5eqN_gPtkkWy0jeZHMHHPMc0j115ML_JR05yS58_RgZB0VqgmDYOY3mxKcijbZ46HPgw_pE2Om6whxDN_UkHLVTpo9DQbhNDPBnLe_ZOixX9by3h55gw=w2200-h863-s-no&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;A place, Dec 2021&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/pw/ADCreHfRAJNr-tZATHdtkH2Or37oinOY2QoqCSw3A7X4oMWfqdDY-eOJjFt0VcZxpx8_87ManjNsOT3Mf-LQ_3K_2idnIwf4lnL842GDY5lkL1Fr8XIppvh8_2-vZdCMdiicxR_xxUMPw2FQztbLW0DMvoyNZA=w1280-h720-s-no&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;Where your paperwork goes, Dec 2021&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/pw/ADCreHfStpFQ_5J6w9I8VlEjbfYyK7dYqwwuWIaKljvx5xSRqO6a9evL8kAJQdhE_x7Fi5sa8vskXIVi_8Aw8n4kMn5INg0aZBgH-evSz4kxG6VlDniAQph4Wb0OXwotG6-VG7iAH2h3-zJ3ZosaHIzfpiTRtQ=w1920-h1080-s-no&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;A day on the job, Jan 2022&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/pw/ADCreHeaRnqYgkisz-DdeqJDfNQQmEheDJgPu5UBaKigM-9cW9PorDRmwPuWnWINnawKfjUzEJvQ2WyjYTOIm8CvkDyOMLLyb5NlxBH17xklIzu8NL8eyucKhtc509qNH1TQyxvBiWOGE39-4hY5ZnI5KvN9Xg=w1920-h1080-s-no&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;Somedays the weather is the perfect gray and you scan your whole town, Feb 2022&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/pw/ADCreHedaEj--Uey2xWXjAUQSosuZJj_qSrkCd7mvxSFbAvNrQA6HUclBx9frUHcMS241AEdUTVH6887qBwFMChJpWm0317T6gvPsNng8sHzMKajZWJyZGAxj9COMZeztTH0yuZ_pFJ7YYpcB1CNaB0rilSXWQ=w2200-h1238-s-no&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;Not the best place to rest, May 2022&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/pw/ADCreHcKRvLD4WW1YudE9vuypGm02bSxPjt5bvQO6qiVt-dcetKuVNHvO2YRC00rFQ5X-5uwh50DOu-hPnTIdwr8V6pW8mdEqSU4bvHOjMkFypvMGKSzDXSUcHpzVNU7qLT3DJGX_xgy-yHTZ6cHNKeeX0KABA=w1920-h1080-s-no&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;Stuff to make more things, Jun 2022&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/pw/ADCreHfqZ9uWYP3KyiWdDDWTubvX2rJ6ZqLkFA143vbzz_-lHvgTnp8s3q6zfJ5k6MRohwLPcQhHTv46wmcgaXH-9WH-W5UBh06WQO8G69Rzr9pGKiW_25Kkcjjt8NchPszsHKSXBwe18CB3lRq86FQZrJ7WLg=w2200-h863-s-no&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;A thing made out of stuff, Jul 2022&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/pw/ADCreHfMMqzudTSFcFOShjD_pPfLohWYboraIQddjVuzZI7oiCDSY2izx8LFYWXbcWoP2VsQK_Nyn_YeRkULkeHtO5iDmj-Gvo4vMX1udEGtroiWzUeMSoogrS1Kpkwdn0OdbqamOEdl-Sk_ToMGo9TRp-Pfaw=w2200-h863-s-no&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;Going some place with the thing ? Jul 2022&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/pw/ADCreHcVoNDBUtAx27yJ4srIPFROIU6cChMq0m9nh4YyYhCh2URRKlR5dXzW5nKpJgSmFmhuTczBz-FDMUDa66jXP7HmC60q2ct4LZw6M32bL74uVz5cosTZfhhQf4SUjpQd63dKmcSpI9YfdvTkN4HyPUd0Lw=w2200-h863-s-no&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;Testing procedural library generation, Aug 2022&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/pw/ADCreHcg3u86VtoNWggahfIYe8Z7hxyiaiBHwK85iac69h7LnJIXXcwmbZZOGdWbrOrC4iBB41e02qN1Im_wm9bMWb9djBUgRFtqaFZp01MZir0NipLvsNNShrlg-gaawLeWGpQlpvzOUken0aroRPRM7su9iQ=w2200-h863-s-no&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;Procedural libraries are quite big, Aug 2022&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/pw/ADCreHettS5hbOqEg-IEQJYvqC1FwMZIqInxiGA_SHexvTIyB5c1ubGkDyU9shHS4CJuF4JT4jyvMnTGdNiRuNby93gmv4CzKEkaPecS5E_RjSUqNzAmCtmWyKh3dVSLNhFKFQJR1S_2aYJDlALGZTnNtpnwUQ=w2200-h863-s-no&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;Trains ! Nov 2022&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/pw/ADCreHeJ6_qOkJHPk6yyqWCfIBRxJTjFOoRTF4qCGRWilvDCj27C0Ih6gMaeJPG1ZL2923zpXype2q-WmVFzO7U1Om5VuDSbJPDaIQU813Il2pzx_l74coj0AqP-jii-ytgux-aFDabPnvmljIabwXqdVTEE-A=w2200-h863-s-no&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;Did I tell you trains are cool ? May 2023&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/pw/ADCreHfGbrWUHIM73Nr68wM2MWo7ahIhpxJqVFTXBJKv_5_s3EZ5gLVR_AGvSWjXQA5oy8p6yHXMq4G6iJCD_aL9pM7NbnidcF005JybdNCZjk36hByM0PBrvHLSOiFaamWw4fviywnwDGYroZHK1nF-vMG9QA=w2200-h863-s-no&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;Trains with boats on top are cooler even, Jun 2023&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/pw/ADCreHcXJGwRgIHcaeg7ZIXQmTPqg9SpdlHTLE6nAmjKviEhkJIwx0yjnXtaXDtEUrbqMo_UfB-_WHYKA6k-YTL-QrKex9qWH5azI1wQ8pVHGfpdJbT1TYO_rHy7JXVptxPETBYrn5GjTIDuqXAGirEgE8EvBw=w2200-h862-s-no&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;Ok this is weird, Jun 2023&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/pw/ADCreHcf_FHnU9M8DKEd43dQIR1_2qRfwNabo9VCm_zPEM1iWto3ltr8gNFShviGo3eYdSzfhhGHAiN2fBC7uRmazMLOdHjbjpzwBrrujMu0fXyjNVECUVE-sY0toG6zV90pVbw9UurpBwUEut10ZMWkn2tI5Q=w2200-h978-s-no&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;Better now, Aug 2023&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/pw/ADCreHc0wJKMv8l596X9x4F-YkJpSXezhfeVnlbSqv1Cy-xh_AkbPv7vJcozRT-XtJkIQPiDG5NgcRyOmsiUKX7VCIKux7A-4zf9GcOHLfDWH4bHL_PVLVX8aMYNVYDHrd-bspCd1xBRT0v4XElh2EMoGozVkg=w2200-h862-s-no&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;Hello there, Jul 2023&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;</content><author><name>J. Mabon</name></author><category term="creative" /><summary type="html">Sometimes you make things but it is never done. Some of it goes here. These images are made on the computer with Blender.</summary></entry><entry><title type="html">Photography I</title><link href="https://www.jmabon.fr//creative/2021/09/03/photography.html" rel="alternate" type="text/html" title="Photography I" /><published>2021-09-03T10:00:00+02:00</published><updated>2021-09-03T10:00:00+02:00</updated><id>https://www.jmabon.fr//creative/2021/09/03/photography</id><content type="html" xml:base="https://www.jmabon.fr//creative/2021/09/03/photography.html">&lt;p&gt;Some shots from 2021, from me and my trusty Sony A7C&lt;/p&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/pw/AM-JKLUdFWdPMjw-tPmcRVH28v7gKnIvFyfyQVpIR69DHqQkzLIyyBoViUasQBwFIjGTdzqAlGnaq1mr8ST6PlgsFFerIowQ0fYAp57ScjeHyGizaU7iEllxxx7zMTrdT7OJBVgGsVI_bM_wlL6fvwtRq3K24Q=w1409-h939-no&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;Moonlight shot, &lt;i&gt;Observatoire Astronomique du plateau de Calern&lt;/i&gt;&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;!-- &lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/pw/AM-JKLVvMIRsGSeukY3nOoOcq3yq39g9UeWkx-iTeKpnNPAUH6Z9uH547DnKPr-Dp-UYhW29f483e6506uYB04QdZYYHP1HjactS2MeajegA4wCQEkjwq64X2T6MjNVe5Xl9ZimL4CBOgAd9FPZqe00DBjzAQA=w1409-h939-no&quot; style=&quot;width:100%&quot;&gt;
    &lt;figcaption&gt;&lt;i&gt;Observatoire Astronomique du plateau de Calern&lt;/i&gt;&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;
 --&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/pw/AM-JKLVrWdxVi-Wa2WeVqMzlrEAcm_NwYG52ePVfUM2_Q3J8pAi6w6JpFXCWwiRNdbfq6UYfxOE9VZyDjAnIh9dKvMsc-hwz9hlT_Us2xktxhxBvn55yrXtFuFTKx_tyWalpvlB4d9zKLlHYOE-j1OfLNdYazw=w1409-h939-no&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;H√©liport, &lt;i&gt;La B√©rarde&lt;/i&gt;&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/pw/AM-JKLXv605462smuz2ZVKMIyBMRQTt-pIkkyBNOTbPvciMwxjts9t1f13OhQFbK0awwMrc0Xb5Dpre_7gONu4V01HAsln2pXz4_GL8gyQgqR8JkQSksj4BRS5gHBPDbYUMOOJSrkgi8boZuwL-gxlYoIAjTfg=w705-h939-no&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;Josette&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/pw/AM-JKLVwsdIpDCG4ZOH6LFMqALuZXHRkS1xE298HCIT6QwBFkWN73V3HU45umzjmYzal9WJyNFHQoNnhUDvYVG1P2Z2oB90bxoaz_mkJhqDPS8C7pkZf_Obe0Z_D88A6RyDVI-mKBgSfBEEchmRRUAWy6Rvq9Q=w1409-h939-no&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;SPARKS !&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/pw/AM-JKLW-Oc3kSkbdO-9DE4f1rdQVq6wVwzYTQ6nRmUddeeyD4Jc0J0x9s_8770I2wPh4QwMFi6UbON30gq3-dwVfFNH3fNlCr3Bq0o8MxxtNn3JbrNbnCxGC-bp55gbPEt1v9EZSEy2h5umo0yLC6ukYexEShw=w705-h939-no&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;&lt;i&gt;The Louvre&lt;/i&gt;&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/pw/AM-JKLX-AXwSWqOEZHBuwuVMlA1u8T7wClWHL9PAGVWQeYFrJx_gLA-DRxxpJnyUPglt9T6rbBJ2cI4xslEHtruFDmPdcxAIj5msHt2P__WqdWK8Lxpv5pU_uYos4B0cz6TIkxrtY7UVC5_-SH_ioBXqq1QkYA=w1409-h939-no&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;&lt;i&gt;Lac du Chambon&lt;/i&gt;&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;!-- &lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/pw/AM-JKLWmOG3gkfTtrWKHdPCvv88_K8DyuUIfUo2nysIdmgTH6Jbe7tpajC0qsfypCbvasreKGDXnsSsc_X05iXisoQsqbVFCE6tmeMscsgAyPpWxkv48gLa8IgIPkPs7dTMsuctBGLaBkuZgt76Bgjl423gHxA=w1409-h939-no&quot; style=&quot;width:100%&quot;&gt;
    &lt;figcaption&gt;&lt;i&gt;Plateau d&apos;Emparis&lt;/i&gt;&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt; --&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/pw/AM-JKLWAuA-DQG84ZKUF43hD2JW28H6FhGJtMu15aRBhcHv9lCPM43HlKrzpQApG5Ri6ZQ0k6DwTeKJLw0JuRFqm9J-fJ7G9H9a7AtbRO_ykZmI6DCixUklhlSS8OvBf-DdVwfYQ0pciOIB8swhAJ59oPM5XBA=w1409-h939-no&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;&lt;i&gt;Plateau d&apos;Emparis&lt;/i&gt;&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/pw/AM-JKLVNFYWP3yr5mEN4NiAruApLgwN_LRpfPbi9Rx2BB0xJp39wwkjpBY99lks3K_TjXxZOH53nOcUMBUqJP_fBG8gHjf8-jAh7Gp_frIlnPFLiIe2UktztQl4tl3hlqvxLIHP_Xw3n6YV_QNZ-zwp6eicgag=w1409-h939-no&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;&lt;i&gt;La B√©rarde&lt;/i&gt;&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/pw/AM-JKLV12ahbqwo3gOBpvlddUY1hYlRz1h1ZBIBo6ZyX9K3Vm8uzAe_GT5_JGqvAp7JfYgCNiVOSSOVgLXeATjHUF9TGCCJ-yTkMv1adr28xGF4PJSMGOGbZgIRI1iSmCqwH6anRLHF2fY3pVXivSrfJsnXokw=w1560-h878-no&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;&lt;i&gt;Huez&lt;/i&gt;&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/pw/AM-JKLVz7je6CE1i2veytOFES0jkzS0CvL0-26fF0eCNb8cB_2yI1Lju35Jd9HtIKs5YwD3lD3sQgg32QWjAdsoHEBg7oeFL1A5u5ttxHZ7QuzzbsLV02xYqAWC42Ye8B7utMMCNEhJfKt4Owt6Z-4_WpfrStQ=w1409-h939-no&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;&lt;i&gt;Somewhere in the kitchen&lt;/i&gt;&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;</content><author><name>J. Mabon</name></author><category term="creative" /><summary type="html">Some shots from 2021, from me and my trusty Sony A7C</summary></entry><entry><title type="html">Spaceship, Ruins, Fishes and stuff</title><link href="https://www.jmabon.fr//creative/2021/02/15/spaceshipruinsandfish.html" rel="alternate" type="text/html" title="Spaceship, Ruins, Fishes and stuff" /><published>2021-02-15T09:00:00+01:00</published><updated>2021-02-15T09:00:00+01:00</updated><id>https://www.jmabon.fr//creative/2021/02/15/spaceshipruinsandfish</id><content type="html" xml:base="https://www.jmabon.fr//creative/2021/02/15/spaceshipruinsandfish.html">&lt;p&gt;A short animation I did with blender, mixing 3D models with toon shaders and the grease pencil tool.
Particle systems are my friends now.&lt;/p&gt;

&lt;style&gt;.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }&lt;/style&gt;
&lt;div class=&quot;embed-container&quot;&gt;&lt;iframe src=&quot;https://www.youtube.com/embed/jh39qB81rX0&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Music :&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Daniel Birch - Broken Surfaces 2&lt;/li&gt;
  &lt;li&gt;Fly, Fly my sadness - The bulgarian voices angelite&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://cdna.artstation.com/p/assets/images/images/034/856/496/large/jules-mabon-previz-09.jpg?1613427588&quot; style=&quot;width:100%&quot; /&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://cdnb.artstation.com/p/assets/images/images/034/856/501/large/jules-mabon-previz-10.jpg?1613427596&quot; style=&quot;width:100%&quot; /&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://cdnb.artstation.com/p/assets/images/images/034/856/503/large/jules-mabon-previz-13.jpg?1613427603&quot; style=&quot;width:100%&quot; /&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://cdna.artstation.com/p/assets/images/images/034/856/492/large/jules-mabon-previz-15.jpg?1613427580&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;Want some funk ?&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;</content><author><name>J. Mabon</name></author><category term="creative" /><summary type="html">A short animation I did with blender, mixing 3D models with toon shaders and the grease pencil tool. Particle systems are my friends now.</summary></entry><entry><title type="html">Royal Dumplings</title><link href="https://www.jmabon.fr//creative/2020/11/21/dumplings.html" rel="alternate" type="text/html" title="Royal Dumplings" /><published>2020-11-21T09:00:00+01:00</published><updated>2020-11-21T09:00:00+01:00</updated><id>https://www.jmabon.fr//creative/2020/11/21/dumplings</id><content type="html" xml:base="https://www.jmabon.fr//creative/2020/11/21/dumplings.html">&lt;p&gt;So I did a small vfx short with the help of my friends. The goal was mainly to test how capable we were of doing such filmy things.&lt;/p&gt;

&lt;!-- &lt;div class=&quot;videoContainer&quot;&gt;
&lt;iframe src=&quot;https://www.youtube.com/embed/IRt39HkI-e8&quot;
frameborder=&quot;0&quot; allowfullscreen class=&quot;video&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt; --&gt;

&lt;style&gt;.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }&lt;/style&gt;
&lt;div class=&quot;embed-container&quot;&gt;&lt;iframe src=&quot;https://www.youtube.com/embed/IRt39HkI-e8&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;h2 id=&quot;a-quick-breakdown&quot;&gt;A quick breakdown&lt;/h2&gt;

&lt;style&gt;.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }&lt;/style&gt;
&lt;div class=&quot;embed-container&quot;&gt;&lt;iframe src=&quot;https://www.youtube.com/embed/JbBbq9M-NzA&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;h2 id=&quot;a-long-breakdown&quot;&gt;A long breakdown&lt;/h2&gt;

&lt;p&gt;It started with me modeling a planter thingy,&lt;/p&gt;

&lt;style&gt;.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }&lt;/style&gt;
&lt;div class=&quot;embed-container&quot;&gt;&lt;iframe src=&quot;https://www.youtube.com/embed/CnqGmPOuvUQ&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;p&gt;Then I thought it would be nice to have a little activity around this thing,
so I did a flying food truck, it took some time&lt;/p&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://cdnb.artstation.com/p/assets/images/images/032/222/803/original/jules-mabon-progress-bato-1.gif?1605806951&quot; style=&quot;width:100%&quot; /&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;style&gt;.embed-container { position: relative; padding-bottom: 56.25%%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }&lt;/style&gt;
&lt;div class=&quot;embed-container&quot;&gt;&lt;iframe src=&quot;https://www.youtube.com/embed/tYHDdHbS5BA&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;!-- &lt;div class=&quot;squareVideoContainer&quot;&gt;
&lt;iframe src=&quot;https://www.youtube.com/embed/tYHDdHbS5BA&quot;
frameborder=&quot;0&quot; allowfullscreen class=&quot;video&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt; --&gt;

&lt;p&gt;Then I realised that if I wanted to make some short sequence out ouf it, I needed to plan for it. So with my friend Alexandre Thery we went to draw a storyboard :
at that point there was no going back&lt;/p&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://cdnb.artstation.com/p/assets/images/images/032/223/733/large/jules-mabon-img-20200530-235351.jpg?1605808731&quot; style=&quot;width:100%&quot; /&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;With the camera moves planned we could proceed to film those people on green screens. At that point I did not have the whole scene properly modeled because we knew that we would made some unexpected change during filming. Waiting for the proper moment to meet I also prepared some props for the shoot, a gyoza box and a bipper-phone-future-thing :&lt;/p&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://cdna.artstation.com/p/assets/images/images/032/223/744/large/jules-mabon-img-20200712-172939.jpg?1605808742&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;The 3D printed black box bipper thingy had some electronics in it so that we could light it up remotely&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://cdna.artstation.com/p/assets/images/images/032/224/280/large/jules-mabon-img-20200712-155533.jpg?1605809716&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;The beautiful mess of filming in your living room&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://cdna.artstation.com/p/assets/images/images/032/224/270/large/jules-mabon-img-20200711-161936.jpg?1605809700&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;We did not have proper green screen supports, but we had some friends ...&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://cdnb.artstation.com/p/assets/images/images/032/224/925/large/jules-mabon-img-20200711-183411.jpg?1605810825&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;...&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;In most cases actors were filmed separately so we could compose them more easily into the scene (also we did not have enough green screen area)&lt;/p&gt;

&lt;p&gt;As anyone might have guessed, filming a cat is not easy, that is not the kind of animal you can ask to stay in some place. So I did a sneaky photoscan of the cat while she was asleep ! It turned out okay-ish except for a moving ear.&lt;/p&gt;

&lt;!-- &lt;div class=&quot;squareVideoContainer&quot;&gt;
&lt;iframe src=&quot;https://cdn-animation.artstation.com/p/video_sources/000/188/402/josette-scan.mp4&quot;
frameborder=&quot;0&quot; allowfullscreen class=&quot;video&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt; --&gt;

&lt;style&gt;.embed-container { position: relative; padding-bottom: 56.25%%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }&lt;/style&gt;
&lt;div class=&quot;embed-container&quot;&gt;&lt;iframe src=&quot;https://cdn-animation.artstation.com/p/video_sources/000/188/402/josette-scan.mp4&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;p&gt;And then I spent months building and texturing the rest of the scene on Blender. There is not much to say about that without getting into too much details and I am tired and lazy.&lt;/p&gt;

&lt;h2 id=&quot;the-things-we-hoped-we-knew&quot;&gt;The things we hoped we knew&lt;/h2&gt;
&lt;p&gt;A bunch of things&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;properly lighting your green screen is sooooooo important (otherwise you will have yourself for it while trying to manually rotoscope people)&lt;/li&gt;
  &lt;li&gt;film wide, then crop, that way you have more freedom while compositing/editing&lt;/li&gt;
  &lt;li&gt;motion tracking is hard, you need to be as steady as possible (no noisy shake) put a waaaaay too muchs markers, and then light you scene properly so that you don‚Äôt get motion blur.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;if you have any questions please contact me on twitter &lt;a href=&quot;https://twitter.com/jul_mab&quot;&gt;@jul_mab&lt;/a&gt; or by engraved marble slates sent over donkey.&lt;/p&gt;

&lt;h2 id=&quot;credits&quot;&gt;Credits&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Editing/sound/storyboard&lt;/strong&gt; : Alexandre Thery&lt;br /&gt;
&lt;strong&gt;CGI/storyboard&lt;/strong&gt; : Jules Mabon (me)&lt;br /&gt;
&lt;strong&gt;Acting&lt;/strong&gt; : Pauline Hu, Lucie Mabon, Antoine Poussot, Josette (the cat) &lt;br /&gt;
&lt;strong&gt;Stuff holding&lt;/strong&gt; : Islaure Herve, Stephane Cardinet&lt;br /&gt;
&lt;strong&gt;Camera supplying&lt;/strong&gt; : Loris Dumont&lt;br /&gt;
&lt;strong&gt;3D printing&lt;/strong&gt; : Arthur Delaitre&lt;br /&gt;
&lt;strong&gt;Music&lt;/strong&gt; : Raphael Rossignol&lt;/p&gt;</content><author><name>J. Mabon</name></author><category term="creative" /><summary type="html">So I did a small vfx short with the help of my friends. The goal was mainly to test how capable we were of doing such filmy things.</summary></entry><entry><title type="html">Stochastic Geometry for Multiple Object Detection and Tracking in High Resolution Multi-Source Data Sets</title><link href="https://www.jmabon.fr//research/2020/10/02/SG-for-MODT.html" rel="alternate" type="text/html" title="Stochastic Geometry for Multiple Object Detection and Tracking in High Resolution Multi-Source Data Sets" /><published>2020-10-02T19:00:00+02:00</published><updated>2020-10-02T19:00:00+02:00</updated><id>https://www.jmabon.fr//research/2020/10/02/SG-for-MODT</id><content type="html" xml:base="https://www.jmabon.fr//research/2020/10/02/SG-for-MODT.html">&lt;p&gt;The following is a detailed description of my PhD subject within the &lt;a href=&quot;https://team.inria.fr/ayana/&quot;&gt;AYANA team at INRIA&lt;/a&gt; under the supervision of &lt;a href=&quot;http://www-sop.inria.fr/members/Josiane.Zerubia/index-eng.html&quot;&gt;Josiane Zerubia&lt;/a&gt; (AYANA team, INRIA) and &lt;a href=&quot;https://team.inria.fr/ayin/dr-mathias-ortner/&quot;&gt;Matias Ortner&lt;/a&gt; (Airbus Defense and Space)&lt;/p&gt;

&lt;p&gt;Unmanned aerial vehicles and low-orbit satellites, including cubesats, are increasingly used for wide area surveillance, which results in large amounts of multi-source that have to be processed and analyzed. These sensor platforms capture vast ground areas at roughly one frame per second. The number of moving objects in such data is typically very high, accounting for up to thousands of objects.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Multiple objects tracking&lt;/strong&gt; has traditionally been a major area of research in the computer vision field, but this type of data poses new, specific, tracking related challenges. The large number of small objects coupled with the reduced frame rate of the video, illumination changes and image registration provide significant sources of errors. Numerous motion models and state estimation methods like the Kalman filter or the particle filter have been proposed for object tracking. 
Classical trackers such as the Multiple Hypothesis Tracker or the Joint Probabilistic Data Association Filter have been employed to solve the data association problem between multiple detections. Both approaches work on a set of data association hypothesis. A strong limitation of these methods is that past decisions cannot be updated when new information is available. One way to cope with this problem is to use a sliding temporal window to perform tracking taking into account both past and future information and hence, removing the causality of the result. A new &lt;strong&gt;spatio-temporal Marked Point Process&lt;/strong&gt; model specifically adapted to the problem of multiple objects tracking has been developed by Craciun et al &lt;a class=&quot;citation&quot; href=&quot;#craciun2015stochastic&quot;&gt;(CrƒÉciun, 2015)&lt;/a&gt;.. Craciun et al. use ellipses to model the objects (boats or cars for instance) adding a non-geometric mark to facilitate the association between objects in different frames.&lt;/p&gt;

&lt;p&gt;Nevertheless, one important drawback of the above-mentioned model is that constant velocity of the moving objects is a necessary prior hypothesis to deal with the corresponding density function to be optimized.&lt;/p&gt;

&lt;p&gt;In this PhD thesis, we propose to both get rid of this constraint by extending the previous model of &lt;a href=&quot;https://team.inria.fr/ayin/paula-craciun/&quot;&gt;Paula Craciun&lt;/a&gt; , and introduce &lt;strong&gt;Machine Learning&lt;/strong&gt; techniques.&lt;/p&gt;

&lt;p&gt;On one hand, &lt;strong&gt;stochastic geometry&lt;/strong&gt; has proven to be extremely powerful for capturing object positions within images using a prior model on their relative positions. For instance, it is possible to introduce a regularizing term that accounts for relative positions of objects, in order to represent specific patterns. One major drawback of this approach is that the amount of marks that can be handled has to be limited in order to avoid an explosion of the problem dimensionality. Moreover, parameter optimization is more complex when objects have complex shapes. 
On the other hand, &lt;strong&gt;deep learning&lt;/strong&gt; approaches have largely proven over the recent years to be extremely efficient in building representations that matches tiny object signatures. Recent work on representations in &lt;strong&gt;latent space&lt;/strong&gt; have shown that it is possible to capture a network interpretation in a small space, even if the network uses a large amount of parameters and operations to infer the likelihood of the presence of an object.&lt;/p&gt;

&lt;p&gt;We propose to merge both approaches and build a Stochastic Point Process model in a state space that is an intermediary layer of a &lt;strong&gt;Convolutional Neural Network&lt;/strong&gt;. Using regularizing terms, it is indeed possible to build &lt;strong&gt;intermediate representations&lt;/strong&gt; in a latent space in which an object of interest such as a car, a boat or a truck, has a simple shape such as a circle, an ellipse or a small segment, which fits perfectly well the &lt;strong&gt;stochastic geometry&lt;/strong&gt; approach.&lt;/p&gt;

&lt;p&gt;After an initial set up on images of cars, trucks, planes and boats, the project will focus on building a &lt;strong&gt;convolutional net&lt;/strong&gt; that projects small sequences of images in a latent space made for &lt;strong&gt;estimation of object positions and velocities&lt;/strong&gt;. Our purpose is to build a &lt;strong&gt;point process model&lt;/strong&gt; in a latent space that captures both objects positions and velocities in order to account for &lt;strong&gt;complex trajectories&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The data and some ground truth will be provided by &lt;strong&gt;Airbus Defense and Space&lt;/strong&gt; (Airbus DS).&lt;/p&gt;

&lt;h4 id=&quot;references&quot;&gt;References&lt;/h4&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;craciun2015stochastic&quot;&gt;CrƒÉciun, P. (2015). &lt;i&gt;Stochastic geometry for automatic multiple object detection and tracking in remotely sensed high resolution image sequences&lt;/i&gt; [PhD thesis].&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;background image from : ESA / Copernicus Sentinel-2A - http://www.esa.int/spaceinimages/Images/2017/04/Central-eastern_Brazil&lt;/p&gt;</content><author><name>J. Mabon</name></author><category term="research" /><summary type="html">The following is a detailed description of my PhD subject within the AYANA team at INRIA under the supervision of Josiane Zerubia (AYANA team, INRIA) and Matias Ortner (Airbus Defense and Space)</summary></entry><entry><title type="html">Mycobacteria tracking on time-lapse imaging</title><link href="https://www.jmabon.fr//research/2020/09/21/Tracking_Cells.html" rel="alternate" type="text/html" title="Mycobacteria tracking on time-lapse imaging" /><published>2020-09-21T14:00:00+02:00</published><updated>2020-09-21T14:00:00+02:00</updated><id>https://www.jmabon.fr//research/2020/09/21/Tracking_Cells</id><content type="html" xml:base="https://www.jmabon.fr//research/2020/09/21/Tracking_Cells.html">&lt;p&gt;During summer 2019 I had the pleasure to do an internship with &lt;a href=&quot;https://www.ebi.ac.uk/research/uhlmann&quot;&gt;Virginie Uhlmann ‚Äòs team&lt;/a&gt; at the &lt;a href=&quot;https://www.ebi.ac.uk&quot;&gt;European Bioinformatics Institute (EBI)&lt;/a&gt; . The team aims at developing tools that blend mathematical models and image processing algorithms to quantitatively characterize the content of bioimages.&lt;/p&gt;

&lt;p&gt;In this post I will discuss the methods used during the course of this internship on &lt;em&gt;multiple cell tracking in time lapse microscopy images&lt;/em&gt;. Most of the methods described in this post were adapted from the cited publications to our case and implemented by me in &lt;strong&gt;Python&lt;/strong&gt; and &lt;strong&gt;Tensorflow&lt;/strong&gt;.&lt;/p&gt;

&lt;h1 id=&quot;goal&quot;&gt;Goal&lt;/h1&gt;

&lt;p&gt;My goal during this 6 months internship was to further develop techniques to track individual &lt;em&gt;Mycobacteria smegmatis&lt;/em&gt; cells in sequences of time-lapse microscopy images.
Since these images are challenging because of &lt;strong&gt;division events&lt;/strong&gt;, &lt;strong&gt;compact cell colonies&lt;/strong&gt; and little prior on individual &lt;strong&gt;cell shapes&lt;/strong&gt;, classical segment-then-track methods are ineffective. We relied on a &lt;strong&gt;graphical model&lt;/strong&gt; solution to solve the &lt;strong&gt;tracking&lt;/strong&gt; and &lt;strong&gt;segmentation&lt;/strong&gt; problem jointly at once on the whole sequence. We also need to identify &lt;strong&gt;division events&lt;/strong&gt; and thus build a &lt;em&gt;Mycobacteria smegmatis&lt;/em&gt; genealogical tree of some sorts.
To build the graphical model, cell &lt;strong&gt;candidates&lt;/strong&gt; must be identified in each individual image. To do so, we explored the use of several convolutional neural network models from U-net to discriminative losses for instance segmentation.&lt;/p&gt;

&lt;h2 id=&quot;mycobacteria-smegmatis&quot;&gt;&lt;em&gt;Mycobacteria smegmatis&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;We were working on time lapse images of &lt;em&gt;Mycobacteria smegmatis&lt;/em&gt; that is a non pathogenic bacterial species that strongly resembles the &lt;em&gt;Mycobacterium tuberculosis&lt;/em&gt;, making it a great tool to study pathogens in a safe environment. Several works &lt;a class=&quot;citation&quot; href=&quot;#Santi2013&quot;&gt;(Santi et al., 2013; Santi &amp;amp; McKinney, 2015)&lt;/a&gt; study the replication mechanism of these cells and how it is linked to antibiotics resistance&lt;/p&gt;

&lt;figure&gt;
	&lt;center&gt;
  	&lt;img src=&quot;https://www.jmabon.fr//img/posts/tracking_cells/mycobacteria.png&quot; alt=&quot;Mycobacteria smegmatis&quot; style=&quot;width:75%&quot; /&gt;
  	&lt;figcaption&gt;Mycobacteria smegmatis in phase microscopy overlayed with fluorescence data &lt;a class=&quot;citation&quot; href=&quot;#ginda2017studies&quot;&gt;(Ginda et al., 2017)&lt;/a&gt;&lt;/figcaption&gt;
  	&lt;/center&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;the-data&quot;&gt;The data&lt;/h2&gt;
&lt;p&gt;The data (courtesy of LMIC, EPFL, Lausanne, Switzerland) consist of several time-lapse microscopy videos showing the growth of M. smegmatis colonies. Images are composed of a phase contrast and a fluorescence channel reporting cellular division. In approximately the first 3/4 of the frames of each videos, the medial axis of the bacterias have been annotated with a &lt;a href=&quot;http://bigwww.epfl.ch/teaching/projects/abstracts/mariani/&quot;&gt;custom software&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;ambiguity-in-single-frame-segmentation&quot;&gt;Ambiguity in single frame segmentation&lt;/h2&gt;
&lt;p&gt;The main issue with this data is that the detection relies mainly on the temporal data since on a given frame it is not obvious to the human eye where the boundaries of the cells are since cells are often cluttered.&lt;/p&gt;
&lt;figure&gt;
	&lt;center&gt;
  	&lt;img src=&quot;https://www.jmabon.fr//img/posts/tracking_cells/detection.png&quot; alt=&quot;Detection ambiguity&quot; style=&quot;width:100%&quot; /&gt;
  	&lt;figcaption&gt;Example of different detection solutions for a single image (the input image in this figure is a substitute of the raw data)&lt;/figcaption&gt;
  	&lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;Here a simple input frame with fews cells can lead to multiple hypothesis on the number and placement of cells in the frame. This shows how there is ambiguity on individual cells detection even in a few cell setup. Later in the time-lapse there can be thousands of cells at once. Therefore, because of the in-frame ambiguity, we cannot simply segment each frame and then track between frames.&lt;/p&gt;

&lt;p&gt;From that example one can intuit that some solutions are more likely than others, also by using the data from the previous and next frame we may have more insight on the most probable solution. For that matter we used a graphical model to make use of all of the data available in order to solve the detection and tracking.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;graphical-models-for-joint-segmentation-and-tracking&quot;&gt;Graphical models for joint Segmentation and tracking&lt;/h1&gt;

&lt;p&gt;Schiegg et al. propose a model based on factor graphs to represent the multiple detection hypothesis and jointly tracking and segmenting on whole videos at once &lt;a class=&quot;citation&quot; href=&quot;#schiegg2014graphical&quot;&gt;(Schiegg et al., 2014)&lt;/a&gt;. To put it in a nutshell, given that we provide a set of &lt;strong&gt;detection candidates&lt;/strong&gt; and &lt;strong&gt;transition candidates&lt;/strong&gt; , along with &lt;strong&gt;associated probabilities&lt;/strong&gt; , this method uses a graph representation to compute the most likely candidates for detections and transitions within some &lt;strong&gt;constraints&lt;/strong&gt; .&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;detection candidates&lt;/strong&gt; are potential cells (for instance the region labeled 1 in the figure, or the union of 2 and 3 labeled 23)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;transition candidates&lt;/strong&gt; represent how these candidates would transition from one frame to the next. Here one could expect 12 (at frame t)to transition into 4 (at frame t+1)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;constraints&lt;/strong&gt; are what makes the solution physically sound, no cell candidates can overlap (cell candidate 12 could not co-exist with 23), cells should not appear and disappear in the middle of the video etc‚Ä¶&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For each of the &lt;strong&gt;detection candidates&lt;/strong&gt; and &lt;strong&gt;transition candidates&lt;/strong&gt; we can provide an associated &lt;strong&gt;probability&lt;/strong&gt;, this is provided by a simple classifier that uses as input some cell data (shape, length etc..) to infer its likelihood of being a cell or transition regardless of context. For instance a one pixel cell would have a very low probability and a transition implying that a cell would travel from one end of the image to the other in a single frame is also rated with a low probability.&lt;/p&gt;

&lt;figure&gt;
	&lt;center&gt;
  	&lt;img src=&quot;https://www.jmabon.fr//img/posts/tracking_cells/graphical_model_complete.png&quot; alt=&quot;A graphical model&quot; style=&quot;width:100%&quot; /&gt;
  	&lt;figcaption&gt;Sample graphical model for only two frames, each &lt;b&gt;detection candidate&lt;/b&gt; is represented by a green node. Yellow nodes correspond to &lt;b&gt;transition candidates&lt;/b&gt; between frames. &lt;b&gt;Constraints&lt;/b&gt; are represented by the square nodes&lt;/figcaption&gt;
  	&lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;To explain the technical functioning of a graphical model would take another presentation, for this time we will focus on how to generate these segmentation proposals. Also this part of the graphical inference model had already been implemented by C. Haubold &lt;a class=&quot;citation&quot; href=&quot;#haubold2017scalable&quot;&gt;(Haubold, 2017)&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;graphical-model-model-limitation&quot;&gt;Graphical model model limitation&lt;/h2&gt;
&lt;p&gt;From the previous example we can expect the graph to grow exponentially as the number cell proposal augments. Thus it is important to generate a set of segmentation proposals that is wide enough to contain the ground truth but small enough not to make the complexity explode.&lt;/p&gt;

&lt;h1 id=&quot;previous-approach&quot;&gt;Previous approach&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;BactImAS&lt;/em&gt; &lt;a class=&quot;citation&quot; href=&quot;#mekterovic2014bactimas&quot;&gt;(Mekteroviƒá et al., 2014)&lt;/a&gt; offers a semi-automated solution to the tracking problem where the user must manually define initial cells, division events, and maybe correct the model‚Äôs prediction in between. To detect cells &lt;em&gt;BactImAS&lt;/em&gt; relies on edge detection, thresholds and skeletonization.
In her PhD thesis &lt;a class=&quot;citation&quot; href=&quot;#uhlmann2017landmark&quot;&gt;(Uhlmann, 2017)&lt;/a&gt;, Virginie Uhlman introduces splines to further automate the process. From the detected cell tips a set of shortest viable path in between are generated as cell proposals. The issue of the exploding graph complexity remains because of the high number of proposals.&lt;/p&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://www.jmabon.fr//img/posts/tracking_cells/uhlmann-prev-appr.jpg&quot; alt=&quot;Uhlmann previous approach&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;V. Uhlmann&apos;s approach. 1 : identify cell tips with pixel classifier, 2 and 3 : model all possible tip to tip links as a shortest path relying on splines &lt;a class=&quot;citation&quot; href=&quot;#uhlmann2017landmark&quot;&gt;(Uhlmann, 2017)&lt;/a&gt;&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;exploring-candidates-generation-through-deep-learning&quot;&gt;Exploring candidates generation through deep learning&lt;/h1&gt;

&lt;p&gt;The aim of my internship project was to explore the generation of cell candidates using deep learning methods. The idea being that a number of cell proposal could be ruled out by their improbable shape, thus adding a learning component (opposed to doing only image processing) to the process could improve the proposals.&lt;/p&gt;

&lt;h2 id=&quot;first-approach--u-net-for-pixel-class-prediction&quot;&gt;First approach : U-net for pixel class prediction&lt;/h2&gt;

&lt;p&gt;Graphical models provide a way to solve for the tracking and segmentation problem simultaneously on the entire video. However, it relies on cell candidates generated for each frame. Since we want the true segmentation to be within the set of candidates, we perform &lt;strong&gt;over-segmentation&lt;/strong&gt; (i.e. we segment too much) to ensure that we obtain more false positives that false negatives (for that we can rule out false positives with the graphical model).
Deep neural networks can be used to generate such segmentation proposals in a robust and generalisable way.
For instance, &lt;a class=&quot;citation&quot; href=&quot;#Falk2019&quot;&gt;(Falk et al., 2019)&lt;/a&gt; propose a deep learning method for detecting instances of cells in images relying on pixel-wise classification. For that matter we used a U-Net architecture.&lt;/p&gt;

&lt;h3 id=&quot;what-is-a-u-net-&quot;&gt;What is a U-net ?&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;U-net&lt;/strong&gt; is a convolutional neural network first introduced by &lt;a class=&quot;citation&quot; href=&quot;#RonnebergerFB15&quot;&gt;(Ronneberger et al., 2015)&lt;/a&gt; for biomedical image segmentation. It aims at providing fine pixel classification with low and high scale awareness of the neighboring pixels. The network is composed of a contracting path that decreases the image size through &lt;strong&gt;convolution&lt;/strong&gt; and &lt;strong&gt;pooling&lt;/strong&gt; while increasing the number of channels, and an expansive path that increases the image size through &lt;strong&gt;convolution&lt;/strong&gt; and &lt;strong&gt;up-sampling&lt;/strong&gt; while decreasing the number of channels. The output image scale  is therefore the same as the input. &lt;strong&gt;Skip paths&lt;/strong&gt; are also added to link layers  within levels, so that the information capturing fine details can be forwarded to the end of the network without traversing the contracting path.&lt;/p&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://www.jmabon.fr//img/posts/tracking_cells/u-net-architecture.png&quot; alt=&quot;Unet Architecture&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;Structure of a basic U-net &lt;a class=&quot;citation&quot; href=&quot;#RonnebergerFB15&quot;&gt;(Ronneberger et al., 2015)&lt;/a&gt;. The &lt;b&gt;skip paths&lt;/b&gt; are shown in gray&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;weighted-soft-max-cross-entropy-loss&quot;&gt;Weighted soft-max cross-entropy loss&lt;/h3&gt;

&lt;p&gt;We use a Cross entropy loss with soft-max as it is commonly used for classification &lt;a class=&quot;citation&quot; href=&quot;#Bishop:2006:PRM:1162264&quot;&gt;(Bishop, 2006)&lt;/a&gt; and can be written as&lt;/p&gt;

\[l(I) := - \sum_{x\in I}w(x) \log
\frac{
\exp \left(    \hat{y}_{y(x)}(x)  \right)
}{
\sum_{k=0}^K\exp \left(    \hat{y}_{k}(x)  \right)
},\]

&lt;p&gt;with \(x\) a pixel in the image domain \(I\), \(\hat{y}_k:I\rightarrow\mathbb{R}\) the predicted score for class \(k\), \(K\) the number of classes, and \(y:I\rightarrow\{0,\dots,K\}\) the ground truth segmentation. Thus, \(\hat{y}_{y(x)}(x)\) corresponds to the predicted score for ground-truth class \(y(x)\) at position \(x\). The \(w(x)\) is a per-pixel weighting term used to handle class imbalance and instance separation, and is defined as&lt;/p&gt;

\[w:=w_\mathrm{bal} +w_\mathrm{sep}\]

&lt;h4 id=&quot;learning-instance-segmentation&quot;&gt;Learning instance segmentation&lt;/h4&gt;

&lt;p&gt;To learn how to segment instances, labels of different instances must be separated by at least one pixel of background, ensuring that each instance is one single connected component. In order for this gap to be predicted correctly by the network, a weighting term \(w_\mathrm{sep}\) is applied on the loss to further penalize errors in boundary areas as&lt;/p&gt;

\[w_\mathrm{sep}(x) := \exp \left( -\frac{(d_1(x)+d_2(x)}{2\sigma^2}\right),\]

&lt;p&gt;with \(d_1\) and \(d_2\) the distances to the two nearest instances. This weight therefore increases at locations where two cells are close together, enforcing a separation. The next figure shows an illustration of  \(w_\mathrm{sep}(x)\) over an image.&lt;/p&gt;

&lt;h4 id=&quot;pixel-class-prediction&quot;&gt;Pixel class prediction&lt;/h4&gt;

&lt;p&gt;Our first objective is to distinguish cells from background, thus we use a background class and a cell class. However individual cells need to be spatially separated by a gap in order to distinguish different entities, hence we introduce an &lt;strong&gt;inner cell&lt;/strong&gt; and &lt;strong&gt;outer cell&lt;/strong&gt; class in order to have separate cell cores between and also not to confuse the classifier by labeling the outer cells as background. Since the cell tips will be a useful information to separate individual cells afterwards we also introduce a &lt;strong&gt;cell tip&lt;/strong&gt; class.&lt;/p&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://www.jmabon.fr//img/posts/tracking_cells/unet-train-weighted.png&quot; alt=&quot;Unet training&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;Training the U-net with our data.&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;Once the U-net is trained, the network outputs probability maps for each class.&lt;/p&gt;

&lt;h3 id=&quot;candidates-generation&quot;&gt;Candidates generation&lt;/h3&gt;

&lt;p&gt;From output of the network we need to produce a set of cell candidates. We devised a method of &lt;em&gt;over-segmentation&lt;/em&gt; to get a set of cell parts that could be assembled to make cell candidates. Basically, from the separated &lt;strong&gt;inner cells&lt;/strong&gt;, we run a watershed algorithm to propagate labels on the cells. We get the results shown below on a sample image. From the over-segmentation output we can build a set of cell proposals. We build proposals from the cell segments by proposing every segment and every union of touching segments (within a set limit amount, so that cells aren‚Äôt too big).&lt;/p&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://www.jmabon.fr//img/posts/tracking_cells/candidates.png&quot; alt=&quot;Candidates generation&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;Building candidates from the predicted pixel classes. Left : the predicted pixel classes. Center : cell segments built from the pixel classification. Right : set of cell candidates built from the available cell segments.&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;
&lt;p&gt;This pixel wise classification approach provided decent results, however the candidates generation part relies heavily on &lt;strong&gt;manually chosen hyper-parameters&lt;/strong&gt;. Also this methods tends to generate quite a lot of very small cell proposals. These numerous false positive make the graphical model complexity explode. Any attempt at choosing a set of hyper-parameters to reduce the number of false positives dramatically increase the number of false negatives, meaning that the graphical model cannot be set up.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;instance-segmentation-with-pixel-embedding&quot;&gt;Instance segmentation with pixel embedding&lt;/h2&gt;
&lt;p&gt;In the first approach the instance segmentation (differentiating one cells from its neighbor) is done by the proxy of &lt;strong&gt;per pixel classification&lt;/strong&gt;. What if we could directly optimize for &lt;strong&gt;instance segmentation&lt;/strong&gt; within the convolutional network and thus minimize the handcrafting of the cell proposal method ?&lt;/p&gt;

&lt;p&gt;We thus focus on a framework in which the instance segmentation problem becomes a &lt;strong&gt;pixel clustering problem&lt;/strong&gt; in a new feature space &lt;a class=&quot;citation&quot; href=&quot;#deBrabandere2017semantic&quot;&gt;(De Brabandere et al., 2017)&lt;/a&gt;. This approach allows predicting instance segmentation with no prior on the number of elements in the image. In constrast to other popular instance segmentation methods like Mask R-CNN &lt;a class=&quot;citation&quot; href=&quot;#he2017mask&quot;&gt;(He et al., 2017)&lt;/a&gt;, it does not rely on region proposal followed by classification. Instead of doing pixel classification with a softmax loss, which would limit the number of instances, we can detect any number of instances in an image can be captured.
Indeed, when predicting instances as one class for each instance, the number of instances is limited by the size of the output vector, i.e. the number of classes. Also
&lt;a class=&quot;citation&quot; href=&quot;#payer2018instance&quot;&gt;(Payer et al., 2018)&lt;/a&gt; extend this idea by considering a tracking component to the problem, adding recurrent components in the network and using a new similarity measure in the feature space.&lt;/p&gt;

&lt;p&gt;In this framework each pixel is attributed a &lt;strong&gt;N dimensional vector&lt;/strong&gt;, so that every pixel from a same cell has a ‚Äúsimilar‚Äù vector and pixels from different cells have ‚Äúnon-similar‚Äù vectors (the similarity measure may depend on the method used). If N=3 you can consider these vectors to be RGB colors, the objective being so that each individual cell is colored uniformly, but different cells have different colors. Through the different figures we project our N dimensional vectors to a 3D space with a PCA so that we can display these vectors as colors. Using the first two dimensions of the PCA we can plot these pixels as points within this 2D space to better visualize the clustering.&lt;/p&gt;

&lt;h3 id=&quot;semantic-instancesegmentation-with-a-discriminative-loss-function&quot;&gt;Semantic InstanceSegmentation with a Discriminative Loss Function&lt;/h3&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://www.jmabon.fr//img/posts/tracking_cells/SIS-DLF-1.jpg&quot; style=&quot;width:100%&quot; /&gt;
    &lt;img src=&quot;https://www.jmabon.fr//img/posts/tracking_cells/SIS-DLF-2.jpg&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt;Top left: the input image. Top right: pixel embeddings projected in a 2D space each car is a cluster. Bottom right: pixel embeddings interpreted as colors, each individual car is colored/projected differently. Bottom right: predicted instances after clustering of the pixel embeddings &lt;a class=&quot;citation&quot; href=&quot;#deBrabandere2017semantic&quot;&gt;(De Brabandere et al., 2017)&lt;/a&gt;&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;h4 id=&quot;discriminative-loss&quot;&gt;discriminative loss&lt;/h4&gt;
&lt;p&gt;To learn the clustering, we rely on a discriminative loss composed of three key parts &lt;a class=&quot;citation&quot; href=&quot;#deBrabandere2017semantic&quot;&gt;(De Brabandere et al., 2017)&lt;/a&gt;. During learning we use as input the image along with ground truth instance labels.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Variance term&lt;/strong&gt;: is an intra-cluster pull force moving embeddings toward the mean of each label cluster. A margin is set for the variance, which corresponds to the inner circle in the next figure. This margin defines how tightly packed clusters should be.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Distance term&lt;/strong&gt;: an inter-cluster push force drawing the mean of embeddings of different instances further
apart from each other. A margin is set for the distance, corresponding to the outer
circle in the next figure. This margin defines how far from each other clusters should be.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Regularization term&lt;/strong&gt;: it is a pull force drawing clusters closer to the origin of the embedding space to avoid values blowing up.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These three part as summed as the loss function and minimized during the learning phase.&lt;/p&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://www.jmabon.fr//img/posts/tracking_cells/SIS-DLF-3.jpg&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt; &lt;a class=&quot;citation&quot; href=&quot;#deBrabandere2017semantic&quot;&gt;(De Brabandere et al., 2017)&lt;/a&gt;&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;clustering&quot;&gt;Clustering&lt;/h3&gt;
&lt;p&gt;For that matter, we used the agglomerative clustering available in sklearn
&lt;a class=&quot;citation&quot; href=&quot;#scikit-learn&quot;&gt;(Pedregosa et al., 2011)&lt;/a&gt;. This method starts by initializing each pixel as a cluster. Clusters are
then merged together in a tree-like manner. The merging stops when a distance threshold is met,
that is when the distance between all clusters is more than the threshold. This is relatable to the
design of the discriminative loss since it enforces a minimal distance between different instances. Also this method does not rely on a prior knowledge of the number of clusters.&lt;/p&gt;

&lt;h3 id=&quot;results-1&quot;&gt;Results&lt;/h3&gt;

&lt;p&gt;Here is the results on a few sample frames. From the second more cluttered frame we observe that this method scales decently to a high number of cells. One improvement might be to add a filter on the smallest size a cluster can be to avoid 1 pixel clusters being proposed as cells.&lt;/p&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://www.jmabon.fr//img/posts/tracking_cells/clustering_18_35.png&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt; Result on an early frame&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;hr /&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://www.jmabon.fr//img/posts/tracking_cells/clustering_18_120.png&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt; Result on a more cluttered frame&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;other-approaches-and-perspectives&quot;&gt;Other approaches and perspectives&lt;/h2&gt;
&lt;p&gt;We tried implementing a Cosine embedding loss &lt;a class=&quot;citation&quot; href=&quot;#payer2018instance&quot;&gt;(Payer et al., 2018)&lt;/a&gt; as a replacement for the discriminative loss function. This yielded similar results but is not as straightforward to cluster as instead of using any N dimensional vector, the pixels are embedded in a N dimensional sphere. Meaning that the similarity function for clustering is not longer a simple euclidean distance but rather a cosine similarity. Also &lt;a class=&quot;citation&quot; href=&quot;#payer2018instance&quot;&gt;(Payer et al., 2018)&lt;/a&gt; proposes to compute the loss over several frame and ensure that cells have a consistent embedding cluster through time, thus learning the &lt;strong&gt;tracking&lt;/strong&gt; part along the &lt;strong&gt;instance segmentation&lt;/strong&gt; part. Due to time constraints I did not manage to properly test this promising method.&lt;/p&gt;

&lt;p&gt;Another idea would be to expand this embedding time consistency to cell divisions, for instance we could devise a loss so that, once projected in a set of specified dimension parent cells would be similar, but remains separate in the other dimensions. If functioning properly this method would negate the need of the graphical model as it solves for &lt;strong&gt;tracking&lt;/strong&gt;, &lt;strong&gt;instance segmentation&lt;/strong&gt; and &lt;strong&gt;cell divisions&lt;/strong&gt; at once.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;From the different convolutional network methods I used I managed to produce strong sets of cell proposals. However due to time constraints and technical limitation of the graphical model, I did not manage to properly use these cell proposals to train the latter. Meanwhile this project gave me a great insight on the functioning and building on convolutional networks.&lt;/p&gt;

&lt;p&gt;Throughout this project, the graphical model was a motivation for our model design choices. A U-net architecture provides an instance segmentation that is not satisfactory as it has extensive post-processing that is specific to the dataset. As we aim for generalizable solution we also explored instance segmentation oriented networks and losses (e.g. discriminative or cosine embedding loss). These offer a more straightforward optimization process to obtain cell candidates for the graphical model. Moreover, the clustering process offers better control on the roughness or finesse of the segmentation candidates.&lt;/p&gt;

&lt;p&gt;Unfortunately, the ground truth matching part which is necessary to train the graphical model, proved to be a roadblock when using our first instance predictions obtained using U-net on a pixel classification task. We sadly did not have time to test ground piping our discriminative network results into the graphical model.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;acknowledgments&quot;&gt;Acknowledgments&lt;/h1&gt;

&lt;p&gt;To the French Embassy in London for funding this internship, to EMBL for making it
possible, to Jos√© for his technical help,
And mostly, warm and sincere thanks to the whole Uhlmann Lab : Virginie, Soham, Yoann,
Johannes, James and Maria.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;bonus-making-some-abstract-art-with-it&quot;&gt;Bonus: Making some abstract ‚Äúart‚Äù with it&lt;/h2&gt;

&lt;figure&gt;
  &lt;center&gt;
  &lt;img src=&quot;https://www.jmabon.fr//img/cells.png&quot; style=&quot;width:100%&quot; /&gt;
  &lt;/center&gt;
&lt;/figure&gt;

&lt;p&gt;As i noticed the network trained on the discriminative loss function outputted some pretty colors I tried to feed it a synthetic image over-packed with cells. This is the result, it makes for a great (but maybe too colorful) wallpaper.&lt;/p&gt;
&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;https://www.jmabon.fr//img/posts/tracking_cells/super_spagettos.png&quot; style=&quot;width:100%&quot; /&gt;
    &lt;figcaption&gt; Left: Input spaghettis. Right: output colors&lt;/figcaption&gt;
    &lt;/center&gt;
&lt;/figure&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;Santi2013&quot;&gt;Santi, I., Dhar, N., Bousbaine, D., Wakamoto, Y., &amp;amp; McKinney, J. D. (2013). Single-cell dynamics of the chromosome replication and cell division cycles in mycobacteria. &lt;i&gt;Nature Communications&lt;/i&gt;, &lt;i&gt;4&lt;/i&gt;, 2470 EP  -. https://doi.org/10.1038/ncomms3470&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Santie01999-14&quot;&gt;Santi, I., &amp;amp; McKinney, J. D. (2015). Chromosome Organization and Replisome Dynamics in Mycobacterium smegmatis. &lt;i&gt;MBio&lt;/i&gt;, &lt;i&gt;6&lt;/i&gt;(1). https://doi.org/10.1128/mBio.01999-14&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;ginda2017studies&quot;&gt;Ginda, K., Santi, I., Bousbaine, D., Zakrzewska-Czerwi≈Ñska, J., Jakimowicz, D., &amp;amp; McKinney, J. (2017). The studies of ParA and ParB dynamics reveal asymmetry of chromosome segregation in mycobacteria. &lt;i&gt;Molecular Microbiology&lt;/i&gt;, &lt;i&gt;105&lt;/i&gt;(3), 453‚Äì468.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;schiegg2014graphical&quot;&gt;Schiegg, M., Hanslovsky, P., Haubold, C., Koethe, U., Hufnagel, L., &amp;amp; Hamprecht, F. A. (2014). Graphical model for joint segmentation and tracking of multiple dividing cells. &lt;i&gt;Bioinformatics&lt;/i&gt;, &lt;i&gt;31&lt;/i&gt;(6), 948‚Äì956.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;haubold2017scalable&quot;&gt;Haubold, C. (2017). &lt;i&gt;Scalable Inference for Multi-target Tracking of Proliferating Cells&lt;/i&gt; [PhD thesis].&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;mekterovic2014bactimas&quot;&gt;Mekteroviƒá, I., Mekteroviƒá, D., &amp;amp; others. (2014). BactImAS: a platform for processing and analysis of bacterial time-lapse microscopy movies. &lt;i&gt;BMC Bioinformatics&lt;/i&gt;, &lt;i&gt;15&lt;/i&gt;(1), 251.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;uhlmann2017landmark&quot;&gt;Uhlmann, V. S. (2017). &lt;i&gt;Landmark active contours for bioimage analysis: A tale of points and curves&lt;/i&gt; [PhD thesis]. Ecole Polytechnique F√©d√©rale de Lausanne.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Falk2019&quot;&gt;Falk, T., Mai, D., Bensch, R., √ái√ßek, √ñ., Abdulkadir, A., Marrakchi, Y., B√∂hm, A., Deubner, J., J√§ckel, Z., Seiwald, K., Dovzhenko, A., Tietz, O., Dal Bosco, C., Walsh, S., Saltukoglu, D., Tay, T. L., Prinz, M., Palme, K., Simons, M., ‚Ä¶ Ronneberger, O. (2019). U-Net: deep learning for cell counting, detection, and morphometry. &lt;i&gt;Nature Methods&lt;/i&gt;, &lt;i&gt;16&lt;/i&gt;(1), 67‚Äì70. https://doi.org/10.1038/s41592-018-0261-2&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;RonnebergerFB15&quot;&gt;Ronneberger, O., Fischer, P., &amp;amp; Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. &lt;i&gt;CoRR&lt;/i&gt;, &lt;i&gt;abs/1505.04597&lt;/i&gt;. http://arxiv.org/abs/1505.04597&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Bishop:2006:PRM:1162264&quot;&gt;Bishop, C. M. (2006). &lt;i&gt;Pattern Recognition and Machine Learning (Information Science and Statistics)&lt;/i&gt;. Springer-Verlag.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;deBrabandere2017semantic&quot;&gt;De Brabandere, B., Neven, D., &amp;amp; Van Gool, L. (2017). Semantic instance segmentation with a discriminative loss function. &lt;i&gt;ArXiv Preprint ArXiv:1708.02551&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;he2017mask&quot;&gt;He, K., Gkioxari, G., Doll√°r, P., &amp;amp; Girshick, R. (2017). Mask r-cnn. &lt;i&gt;Proceedings of the IEEE International Conference on Computer Vision&lt;/i&gt;, 2961‚Äì2969.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;payer2018instance&quot;&gt;Payer, C., ≈†tern, D., Neff, T., Bischof, H., &amp;amp; Urschler, M. (2018). Instance segmentation and tracking with cosine embeddings and recurrent hourglass networks. &lt;i&gt;International Conference on Medical Image Computing and Computer-Assisted Intervention&lt;/i&gt;, 3‚Äì11.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;scikit-learn&quot;&gt;Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., &amp;amp; Duchesnay, E. (2011). Scikit-learn: Machine Learning in Python. &lt;i&gt;Journal of Machine Learning Research&lt;/i&gt;, &lt;i&gt;12&lt;/i&gt;, 2825‚Äì2830.&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name>J. Mabon</name></author><category term="research" /><summary type="html">During summer 2019 I had the pleasure to do an internship with Virginie Uhlmann ‚Äòs team at the European Bioinformatics Institute (EBI) . The team aims at developing tools that blend mathematical models and image processing algorithms to quantitatively characterize the content of bioimages. In this post I will discuss the methods used during the course of this internship on multiple cell tracking in time lapse microscopy images. Most of the methods described in this post were adapted from the cited publications to our case and implemented by me in Python and Tensorflow. Goal My goal during this 6 months internship was to further develop techniques to track individual Mycobacteria smegmatis cells in sequences of time-lapse microscopy images. Since these images are challenging because of division events, compact cell colonies and little prior on individual cell shapes, classical segment-then-track methods are ineffective. We relied on a graphical model solution to solve the tracking and segmentation problem jointly at once on the whole sequence. We also need to identify division events and thus build a Mycobacteria smegmatis genealogical tree of some sorts. To build the graphical model, cell candidates must be identified in each individual image. To do so, we explored the use of several convolutional neural network models from U-net to discriminative losses for instance segmentation. Mycobacteria smegmatis We were working on time lapse images of Mycobacteria smegmatis that is a non pathogenic bacterial species that strongly resembles the Mycobacterium tuberculosis, making it a great tool to study pathogens in a safe environment. Several works (Santi et al., 2013; Santi &amp;amp; McKinney, 2015) study the replication mechanism of these cells and how it is linked to antibiotics resistance Mycobacteria smegmatis in phase microscopy overlayed with fluorescence data (Ginda et al., 2017) The data The data (courtesy of LMIC, EPFL, Lausanne, Switzerland) consist of several time-lapse microscopy videos showing the growth of M. smegmatis colonies. Images are composed of a phase contrast and a fluorescence channel reporting cellular division. In approximately the first 3/4 of the frames of each videos, the medial axis of the bacterias have been annotated with a custom software. Ambiguity in single frame segmentation The main issue with this data is that the detection relies mainly on the temporal data since on a given frame it is not obvious to the human eye where the boundaries of the cells are since cells are often cluttered. Example of different detection solutions for a single image (the input image in this figure is a substitute of the raw data) Here a simple input frame with fews cells can lead to multiple hypothesis on the number and placement of cells in the frame. This shows how there is ambiguity on individual cells detection even in a few cell setup. Later in the time-lapse there can be thousands of cells at once. Therefore, because of the in-frame ambiguity, we cannot simply segment each frame and then track between frames. From that example one can intuit that some solutions are more likely than others, also by using the data from the previous and next frame we may have more insight on the most probable solution. For that matter we used a graphical model to make use of all of the data available in order to solve the detection and tracking. Graphical models for joint Segmentation and tracking Schiegg et al. propose a model based on factor graphs to represent the multiple detection hypothesis and jointly tracking and segmenting on whole videos at once (Schiegg et al., 2014). To put it in a nutshell, given that we provide a set of detection candidates and transition candidates , along with associated probabilities , this method uses a graph representation to compute the most likely candidates for detections and transitions within some constraints . detection candidates are potential cells (for instance the region labeled 1 in the figure, or the union of 2 and 3 labeled 23) transition candidates represent how these candidates would transition from one frame to the next. Here one could expect 12 (at frame t)to transition into 4 (at frame t+1) constraints are what makes the solution physically sound, no cell candidates can overlap (cell candidate 12 could not co-exist with 23), cells should not appear and disappear in the middle of the video etc‚Ä¶ For each of the detection candidates and transition candidates we can provide an associated probability, this is provided by a simple classifier that uses as input some cell data (shape, length etc..) to infer its likelihood of being a cell or transition regardless of context. For instance a one pixel cell would have a very low probability and a transition implying that a cell would travel from one end of the image to the other in a single frame is also rated with a low probability. Sample graphical model for only two frames, each detection candidate is represented by a green node. Yellow nodes correspond to transition candidates between frames. Constraints are represented by the square nodes To explain the technical functioning of a graphical model would take another presentation, for this time we will focus on how to generate these segmentation proposals. Also this part of the graphical inference model had already been implemented by C. Haubold (Haubold, 2017). Graphical model model limitation From the previous example we can expect the graph to grow exponentially as the number cell proposal augments. Thus it is important to generate a set of segmentation proposals that is wide enough to contain the ground truth but small enough not to make the complexity explode. Previous approach BactImAS (Mekteroviƒá et al., 2014) offers a semi-automated solution to the tracking problem where the user must manually define initial cells, division events, and maybe correct the model‚Äôs prediction in between. To detect cells BactImAS relies on edge detection, thresholds and skeletonization. In her PhD thesis (Uhlmann, 2017), Virginie Uhlman introduces splines to further automate the process. From the detected cell tips a set of shortest viable path in between are generated as cell proposals. The issue of the exploding graph complexity remains because of the high number of proposals. V. Uhlmann&apos;s approach. 1 : identify cell tips with pixel classifier, 2 and 3 : model all possible tip to tip links as a shortest path relying on splines (Uhlmann, 2017) Exploring candidates generation through deep learning The aim of my internship project was to explore the generation of cell candidates using deep learning methods. The idea being that a number of cell proposal could be ruled out by their improbable shape, thus adding a learning component (opposed to doing only image processing) to the process could improve the proposals. First approach : U-net for pixel class prediction Graphical models provide a way to solve for the tracking and segmentation problem simultaneously on the entire video. However, it relies on cell candidates generated for each frame. Since we want the true segmentation to be within the set of candidates, we perform over-segmentation (i.e. we segment too much) to ensure that we obtain more false positives that false negatives (for that we can rule out false positives with the graphical model). Deep neural networks can be used to generate such segmentation proposals in a robust and generalisable way. For instance, (Falk et al., 2019) propose a deep learning method for detecting instances of cells in images relying on pixel-wise classification. For that matter we used a U-Net architecture. What is a U-net ? U-net is a convolutional neural network first introduced by (Ronneberger et al., 2015) for biomedical image segmentation. It aims at providing fine pixel classification with low and high scale awareness of the neighboring pixels. The network is composed of a contracting path that decreases the image size through convolution and pooling while increasing the number of channels, and an expansive path that increases the image size through convolution and up-sampling while decreasing the number of channels. The output image scale is therefore the same as the input. Skip paths are also added to link layers within levels, so that the information capturing fine details can be forwarded to the end of the network without traversing the contracting path. Structure of a basic U-net (Ronneberger et al., 2015). The skip paths are shown in gray Weighted soft-max cross-entropy loss We use a Cross entropy loss with soft-max as it is commonly used for classification (Bishop, 2006) and can be written as \[l(I) := - \sum_{x\in I}w(x) \log \frac{ \exp \left( \hat{y}_{y(x)}(x) \right) }{ \sum_{k=0}^K\exp \left( \hat{y}_{k}(x) \right) },\] with \(x\) a pixel in the image domain \(I\), \(\hat{y}_k:I\rightarrow\mathbb{R}\) the predicted score for class \(k\), \(K\) the number of classes, and \(y:I\rightarrow\{0,\dots,K\}\) the ground truth segmentation. Thus, \(\hat{y}_{y(x)}(x)\) corresponds to the predicted score for ground-truth class \(y(x)\) at position \(x\). The \(w(x)\) is a per-pixel weighting term used to handle class imbalance and instance separation, and is defined as \[w:=w_\mathrm{bal} +w_\mathrm{sep}\] Learning instance segmentation To learn how to segment instances, labels of different instances must be separated by at least one pixel of background, ensuring that each instance is one single connected component. In order for this gap to be predicted correctly by the network, a weighting term \(w_\mathrm{sep}\) is applied on the loss to further penalize errors in boundary areas as \[w_\mathrm{sep}(x) := \exp \left( -\frac{(d_1(x)+d_2(x)}{2\sigma^2}\right),\] with \(d_1\) and \(d_2\) the distances to the two nearest instances. This weight therefore increases at locations where two cells are close together, enforcing a separation. The next figure shows an illustration of \(w_\mathrm{sep}(x)\) over an image. Pixel class prediction Our first objective is to distinguish cells from background, thus we use a background class and a cell class. However individual cells need to be spatially separated by a gap in order to distinguish different entities, hence we introduce an inner cell and outer cell class in order to have separate cell cores between and also not to confuse the classifier by labeling the outer cells as background. Since the cell tips will be a useful information to separate individual cells afterwards we also introduce a cell tip class. Training the U-net with our data. Once the U-net is trained, the network outputs probability maps for each class. Candidates generation From output of the network we need to produce a set of cell candidates. We devised a method of over-segmentation to get a set of cell parts that could be assembled to make cell candidates. Basically, from the separated inner cells, we run a watershed algorithm to propagate labels on the cells. We get the results shown below on a sample image. From the over-segmentation output we can build a set of cell proposals. We build proposals from the cell segments by proposing every segment and every union of touching segments (within a set limit amount, so that cells aren‚Äôt too big). Building candidates from the predicted pixel classes. Left : the predicted pixel classes. Center : cell segments built from the pixel classification. Right : set of cell candidates built from the available cell segments. Results This pixel wise classification approach provided decent results, however the candidates generation part relies heavily on manually chosen hyper-parameters. Also this methods tends to generate quite a lot of very small cell proposals. These numerous false positive make the graphical model complexity explode. Any attempt at choosing a set of hyper-parameters to reduce the number of false positives dramatically increase the number of false negatives, meaning that the graphical model cannot be set up. Instance segmentation with pixel embedding In the first approach the instance segmentation (differentiating one cells from its neighbor) is done by the proxy of per pixel classification. What if we could directly optimize for instance segmentation within the convolutional network and thus minimize the handcrafting of the cell proposal method ? We thus focus on a framework in which the instance segmentation problem becomes a pixel clustering problem in a new feature space (De Brabandere et al., 2017). This approach allows predicting instance segmentation with no prior on the number of elements in the image. In constrast to other popular instance segmentation methods like Mask R-CNN (He et al., 2017), it does not rely on region proposal followed by classification. Instead of doing pixel classification with a softmax loss, which would limit the number of instances, we can detect any number of instances in an image can be captured. Indeed, when predicting instances as one class for each instance, the number of instances is limited by the size of the output vector, i.e. the number of classes. Also (Payer et al., 2018) extend this idea by considering a tracking component to the problem, adding recurrent components in the network and using a new similarity measure in the feature space. In this framework each pixel is attributed a N dimensional vector, so that every pixel from a same cell has a ‚Äúsimilar‚Äù vector and pixels from different cells have ‚Äúnon-similar‚Äù vectors (the similarity measure may depend on the method used). If N=3 you can consider these vectors to be RGB colors, the objective being so that each individual cell is colored uniformly, but different cells have different colors. Through the different figures we project our N dimensional vectors to a 3D space with a PCA so that we can display these vectors as colors. Using the first two dimensions of the PCA we can plot these pixels as points within this 2D space to better visualize the clustering. Semantic InstanceSegmentation with a Discriminative Loss Function Top left: the input image. Top right: pixel embeddings projected in a 2D space each car is a cluster. Bottom right: pixel embeddings interpreted as colors, each individual car is colored/projected differently. Bottom right: predicted instances after clustering of the pixel embeddings (De Brabandere et al., 2017) discriminative loss To learn the clustering, we rely on a discriminative loss composed of three key parts (De Brabandere et al., 2017). During learning we use as input the image along with ground truth instance labels. Variance term: is an intra-cluster pull force moving embeddings toward the mean of each label cluster. A margin is set for the variance, which corresponds to the inner circle in the next figure. This margin defines how tightly packed clusters should be. Distance term: an inter-cluster push force drawing the mean of embeddings of different instances further apart from each other. A margin is set for the distance, corresponding to the outer circle in the next figure. This margin defines how far from each other clusters should be. Regularization term: it is a pull force drawing clusters closer to the origin of the embedding space to avoid values blowing up. These three part as summed as the loss function and minimized during the learning phase. (De Brabandere et al., 2017) Clustering For that matter, we used the agglomerative clustering available in sklearn (Pedregosa et al., 2011). This method starts by initializing each pixel as a cluster. Clusters are then merged together in a tree-like manner. The merging stops when a distance threshold is met, that is when the distance between all clusters is more than the threshold. This is relatable to the design of the discriminative loss since it enforces a minimal distance between different instances. Also this method does not rely on a prior knowledge of the number of clusters. Results Here is the results on a few sample frames. From the second more cluttered frame we observe that this method scales decently to a high number of cells. One improvement might be to add a filter on the smallest size a cluster can be to avoid 1 pixel clusters being proposed as cells. Result on an early frame Result on a more cluttered frame Other approaches and perspectives We tried implementing a Cosine embedding loss (Payer et al., 2018) as a replacement for the discriminative loss function. This yielded similar results but is not as straightforward to cluster as instead of using any N dimensional vector, the pixels are embedded in a N dimensional sphere. Meaning that the similarity function for clustering is not longer a simple euclidean distance but rather a cosine similarity. Also (Payer et al., 2018) proposes to compute the loss over several frame and ensure that cells have a consistent embedding cluster through time, thus learning the tracking part along the instance segmentation part. Due to time constraints I did not manage to properly test this promising method. Another idea would be to expand this embedding time consistency to cell divisions, for instance we could devise a loss so that, once projected in a set of specified dimension parent cells would be similar, but remains separate in the other dimensions. If functioning properly this method would negate the need of the graphical model as it solves for tracking, instance segmentation and cell divisions at once. Conclusion From the different convolutional network methods I used I managed to produce strong sets of cell proposals. However due to time constraints and technical limitation of the graphical model, I did not manage to properly use these cell proposals to train the latter. Meanwhile this project gave me a great insight on the functioning and building on convolutional networks. Throughout this project, the graphical model was a motivation for our model design choices. A U-net architecture provides an instance segmentation that is not satisfactory as it has extensive post-processing that is specific to the dataset. As we aim for generalizable solution we also explored instance segmentation oriented networks and losses (e.g. discriminative or cosine embedding loss). These offer a more straightforward optimization process to obtain cell candidates for the graphical model. Moreover, the clustering process offers better control on the roughness or finesse of the segmentation candidates. Unfortunately, the ground truth matching part which is necessary to train the graphical model, proved to be a roadblock when using our first instance predictions obtained using U-net on a pixel classification task. We sadly did not have time to test ground piping our discriminative network results into the graphical model. Acknowledgments To the French Embassy in London for funding this internship, to EMBL for making it possible, to Jos√© for his technical help, And mostly, warm and sincere thanks to the whole Uhlmann Lab : Virginie, Soham, Yoann, Johannes, James and Maria. Bonus: Making some abstract ‚Äúart‚Äù with it As i noticed the network trained on the discriminative loss function outputted some pretty colors I tried to feed it a synthetic image over-packed with cells. This is the result, it makes for a great (but maybe too colorful) wallpaper. Left: Input spaghettis. Right: output colors References Santi, I., Dhar, N., Bousbaine, D., Wakamoto, Y., &amp;amp; McKinney, J. D. (2013). Single-cell dynamics of the chromosome replication and cell division cycles in mycobacteria. Nature Communications, 4, 2470 EP -. https://doi.org/10.1038/ncomms3470 Santi, I., &amp;amp; McKinney, J. D. (2015). Chromosome Organization and Replisome Dynamics in Mycobacterium smegmatis. MBio, 6(1). https://doi.org/10.1128/mBio.01999-14 Ginda, K., Santi, I., Bousbaine, D., Zakrzewska-Czerwi≈Ñska, J., Jakimowicz, D., &amp;amp; McKinney, J. (2017). The studies of ParA and ParB dynamics reveal asymmetry of chromosome segregation in mycobacteria. Molecular Microbiology, 105(3), 453‚Äì468. Schiegg, M., Hanslovsky, P., Haubold, C., Koethe, U., Hufnagel, L., &amp;amp; Hamprecht, F. A. (2014). Graphical model for joint segmentation and tracking of multiple dividing cells. Bioinformatics, 31(6), 948‚Äì956. Haubold, C. (2017). Scalable Inference for Multi-target Tracking of Proliferating Cells [PhD thesis]. Mekteroviƒá, I., Mekteroviƒá, D., &amp;amp; others. (2014). BactImAS: a platform for processing and analysis of bacterial time-lapse microscopy movies. BMC Bioinformatics, 15(1), 251. Uhlmann, V. S. (2017). Landmark active contours for bioimage analysis: A tale of points and curves [PhD thesis]. Ecole Polytechnique F√©d√©rale de Lausanne. Falk, T., Mai, D., Bensch, R., √ái√ßek, √ñ., Abdulkadir, A., Marrakchi, Y., B√∂hm, A., Deubner, J., J√§ckel, Z., Seiwald, K., Dovzhenko, A., Tietz, O., Dal Bosco, C., Walsh, S., Saltukoglu, D., Tay, T. L., Prinz, M., Palme, K., Simons, M., ‚Ä¶ Ronneberger, O. (2019). U-Net: deep learning for cell counting, detection, and morphometry. Nature Methods, 16(1), 67‚Äì70. https://doi.org/10.1038/s41592-018-0261-2 Ronneberger, O., Fischer, P., &amp;amp; Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. CoRR, abs/1505.04597. http://arxiv.org/abs/1505.04597 Bishop, C. M. (2006). Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-Verlag. De Brabandere, B., Neven, D., &amp;amp; Van Gool, L. (2017). Semantic instance segmentation with a discriminative loss function. ArXiv Preprint ArXiv:1708.02551. He, K., Gkioxari, G., Doll√°r, P., &amp;amp; Girshick, R. (2017). Mask r-cnn. Proceedings of the IEEE International Conference on Computer Vision, 2961‚Äì2969. Payer, C., ≈†tern, D., Neff, T., Bischof, H., &amp;amp; Urschler, M. (2018). Instance segmentation and tracking with cosine embeddings and recurrent hourglass networks. International Conference on Medical Image Computing and Computer-Assisted Intervention, 3‚Äì11. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., &amp;amp; Duchesnay, E. (2011). Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 12, 2825‚Äì2830.</summary></entry><entry><title type="html">Welcome !</title><link href="https://www.jmabon.fr//research/2020/09/02/welcome.html" rel="alternate" type="text/html" title="Welcome !" /><published>2020-09-02T14:32:11+02:00</published><updated>2020-09-02T14:32:11+02:00</updated><id>https://www.jmabon.fr//research/2020/09/02/welcome</id><content type="html" xml:base="https://www.jmabon.fr//research/2020/09/02/welcome.html">&lt;p&gt;Welcome to my new website. It is quite empty for now, some content might be added in the near future. In the mean time please stare silently into the void of this page.&lt;/p&gt;</content><author><name>J. Mabon</name></author><category term="research" /><summary type="html">Welcome to my new website. It is quite empty for now, some content might be added in the near future. In the mean time please stare silently into the void of this page.</summary></entry><entry><title type="html">Desert Spider animation</title><link href="https://www.jmabon.fr//creative/2020/07/21/desert-spider.html" rel="alternate" type="text/html" title="Desert Spider animation" /><published>2020-07-21T16:00:00+02:00</published><updated>2020-07-21T16:00:00+02:00</updated><id>https://www.jmabon.fr//creative/2020/07/21/desert-spider</id><content type="html" xml:base="https://www.jmabon.fr//creative/2020/07/21/desert-spider.html">&lt;p&gt;Another Blender grease pencil and 3D animation. This time with more rigging.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;560&quot; src=&quot;https://www.youtube.com/embed/eMEvfReUyPk&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;The rig even adapts proceduraly to the terrain height&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://cdn-animation.artstation.com/p/video_sources/000/100/912/legz-2.mp4&quot;&gt;&lt;/iframe&gt;</content><author><name>J. Mabon</name></author><category term="creative" /><summary type="html">Another Blender grease pencil and 3D animation. This time with more rigging. The rig even adapts proceduraly to the terrain height</summary></entry></feed>